{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"WIrmPJxmmm8W"},"outputs":[],"source":["import pandas as pd\n","modeling_df = pd.read_csv('modeling_df.csv',index_col=0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pQ2aXNWuTL68"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(modeling_df.iloc[:,[0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56]],modeling_df['result'],test_size=0.25,stratify=modeling_df['result'], random_state=123)\n","X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.3, stratify = y_test, random_state=123)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YD9hv9ypTL68","outputId":"313697c5-bac2-46d3-d06a-257f841e4bf3"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'pycaret'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32mc:\\Users\\dlwjd\\내 드라이브\\임시\\sk학원\\과제\\모의 프로젝트\\modeling_찐.ipynb 셀 3\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dlwjd/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/%EC%9E%84%EC%8B%9C/sk%ED%95%99%EC%9B%90/%EA%B3%BC%EC%A0%9C/%EB%AA%A8%EC%9D%98%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/modeling_%EC%B0%90.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Importing dataset\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/dlwjd/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/%EC%9E%84%EC%8B%9C/sk%ED%95%99%EC%9B%90/%EA%B3%BC%EC%A0%9C/%EB%AA%A8%EC%9D%98%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/modeling_%EC%B0%90.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpycaret\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m get_data\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dlwjd/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/%EC%9E%84%EC%8B%9C/sk%ED%95%99%EC%9B%90/%EA%B3%BC%EC%A0%9C/%EB%AA%A8%EC%9D%98%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/modeling_%EC%B0%90.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m diabetes \u001b[39m=\u001b[39m get_data(\u001b[39m'\u001b[39m\u001b[39mdiabetes\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dlwjd/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/%EC%9E%84%EC%8B%9C/sk%ED%95%99%EC%9B%90/%EA%B3%BC%EC%A0%9C/%EB%AA%A8%EC%9D%98%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/modeling_%EC%B0%90.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Importing module and initializing setup\u001b[39;00m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pycaret'"]}],"source":["# Importing dataset\n","from pycaret.datasets import get_data\n","diabetes = get_data('diabetes')\n","\n","# Importing module and initializing setup\n","from pycaret.classification import *\n","clf1 = setup(data = diabetes, target = 'Class variable')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQmP5oRzmm8c"},"outputs":[],"source":["#knn\n","import optuna, matplotlib.pyplot as plt\n","from sklearn.model_selection import KFold, cross_val_score\n","from sklearn.metrics import accuracy_score, precision_score, recall_score\n","from sklearn.neighbors import KNeighborsClassifier\n","#1 k-fold\n","kfold = KFold(n_splits=5,random_state=123, shuffle=True)\n","#2 knn\n","def knn_objective(trial):\n","    params= {'algorithm' :  trial.suggest_categorical('algorithm', ['auto','ball_tree','kd_tree','brute']),\n","             'n_neighbors' : trial.suggest_int('n_neighbors', 3, 13, step=2)\n","            }\n","    knn = KNeighborsClassifier(n_jobs=-1, **params)\n","    score = cross_val_score(knn, X_train, y_train, cv=kfold, scoring=\"accuracy\")\n","    accuracy = score.mean()\n","    return accuracy\n","knn_study = optuna.create_study(direction=\"maximize\")\n","knn_study.optimize(knn_objective, n_trials=50)\n","knn_study.optimize(knn_objective, n_trials=10)\n","knn_study.best_trial.params\n","knn_study.best_trial.values\n","\n","print(optuna.visualization.plot_param_importances(knn_study))\n","opt_rf = KNeighborsClassifier(n_jobs=-1, random_state=123,\n","                                criterion = knn_study.best_params[\"algorithm\"],\n","                                n_neighbors = knn_study.best_params[\"n_neighbors\"] #fix overfitting\n","                                )\n","opt_rf.fit(X_train, y_train)\n","print(accuracy_score(y_test, opt_rf.predict(X_test)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KHLeJNZnmm8d"},"outputs":[],"source":["#RandomForest\n","import optuna, matplotlib.pyplot as plt\n","from sklearn.model_selection import KFold, cross_val_score\n","from sklearn.metrics import accuracy_score, precision_score, recall_score\n","from sklearn.ensemble import RandomForestClassifier\n","#1 k-fold\n","kfold = KFold(n_splits=5, random_state=123, shuffle=True)\n","#2 RandomForest\n","def rf_objective(trial):\n","    params = {\n","        \"criterion\" : trial.suggest_categorical(\"criterion\", [\"gini\",\"entropy\"]),\n","        \"n_estimators\" : trial.suggest_int(\"n_estimators\", 100,500),\n","        \"max_depth\" : trial.suggest_int(\"max_depth\", 1, 100),\n","        \"max_leaf_nodes\":trial.suggest_int(\"max_leaf_nodes\", 2, 1000),\n","        \"max_features\" : trial.suggest_int(\"max_features\", 1, X_train.shape[1]),\n","        \"min_samples_leaf\":trial.suggest_int(\"min_samples_leaf\", 1, 50)\n","    }\n","    rf = RandomForestClassifier(n_jobs=-1, random_state=123, **params)\n","    score = cross_val_score(rf, X_train, y_train, cv=kfold, scoring=\"accuracy\")\n","    accuracy = score.mean()\n","    return accuracy\n","rf_study = optuna.create_study(direction=\"maximize\")\n","print(rf_study.optimize(rf_objective, n_trials=10))\n","print(rf_study.best_trial.params)\n","print(rf_study.best_trial.values)\n","print(optuna.visualization.plot_param_importances(rf_study))\n","#3 optimization\n","opt_rf = RandomForestClassifier(n_jobs=-1,\n","                                random_state=123,\n","                                criterion = rf_study.best_params[\"criterion\"],\n","                                n_estimators = rf_study.best_params[\"n_estimators\"],\n","                                max_depth = rf_study.best_params[\"max_depth\"],\n","                                max_leaf_nodes = rf_study.best_params[\"max_leaf_nodes\"],\n","                                max_features = rf_study.best_params[\"max_features\"],\n","                                min_samples_leaf = rf_study.best_params[\"min_samples_leaf\"]\n","                                )\n","opt_rf.fit(X_train, y_train)\n","print(accuracy_score(y_test, opt_rf.predict(X_test)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fWQbOOqCmm8f","outputId":"0cf820b7-fab1-4f24-9f29-fefecfd2c3e3"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-11-16 17:06:42,265]\u001b[0m A new study created in memory with name: no-name-13eb0c28-4701-4a97-b12c-75bdc4f78da9\u001b[0m\n","C:\\Users\\dlwjd\\AppData\\Local\\Temp\\ipykernel_1076\\1500963839.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  \"learning_rate\" : trial.suggest_loguniform(\"learning_rate\", 1e-3, 8e-1),\n","C:\\Users\\dlwjd\\AppData\\Local\\Temp\\ipykernel_1076\\1500963839.py:18: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  \"subsample\" : trial.suggest_discrete_uniform(\"subsample\", 0.1, 1, 0.05),\n","C:\\Users\\dlwjd\\AppData\\Local\\Temp\\ipykernel_1076\\1500963839.py:19: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  \"colsample_bytree\":trial.suggest_discrete_uniform(\"colsample_bytree\", 0.1,1, 0.05),\n","C:\\Users\\dlwjd\\AppData\\Local\\Temp\\ipykernel_1076\\1500963839.py:22: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  \"reg_alpha\" : trial.suggest_loguniform(\"reg_alpha\", 1e-2, 1e1),\n","C:\\Users\\dlwjd\\AppData\\Local\\Temp\\ipykernel_1076\\1500963839.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  \"reg_lambda\" : trial.suggest_loguniform(\"reg_lambda\", 1e-2, 1e1),\n","c:\\Users\\dlwjd\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1421: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n","  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n","c:\\Users\\dlwjd\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1421: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n","  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n","c:\\Users\\dlwjd\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1421: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n","  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"]}],"source":["from sklearn.preprocessing import LabelEncoder\n","le = LabelEncoder()\n","y_train = le.fit_transform(y_train)\n","#xgboost\n","import optuna, matplotlib.pyplot as plt\n","from sklearn.model_selection import KFold, cross_val_score\n","from sklearn.metrics import accuracy_score, precision_score, recall_score\n","from xgboost import XGBClassifier\n","#1 k-fold\n","kfold = KFold(n_splits=5,random_state=123, shuffle=True)\n","#2 xgboost\n","def xgb_objective(trial):\n","    params = {\n","        \"booster\" : trial.suggest_categorical(\"booster\", [\"gbtree\",\"gblinear\",\"dart\"]),\n","        \"n_estimators\" : trial.suggest_int(\"n_estimators\", 100,500),\n","        \"learning_rate\" : trial.suggest_loguniform(\"learning_rate\", 1e-3, 8e-1),\n","        \"max_depth\" : trial.suggest_int(\"max_depth\", 1, 100),\n","        \"subsample\" : trial.suggest_discrete_uniform(\"subsample\", 0.1, 1, 0.05),\n","        \"colsample_bytree\":trial.suggest_discrete_uniform(\"colsample_bytree\", 0.1,1, 0.05),\n","        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.5, 1.0),\n","        \"colsample_bynode\": trial.suggest_float(\"colsample_bynode\", 0.5, 1.0),\n","        \"reg_alpha\" : trial.suggest_loguniform(\"reg_alpha\", 1e-2, 1e1),\n","        \"reg_lambda\" : trial.suggest_loguniform(\"reg_lambda\", 1e-2, 1e1),\n","        'min_child_weight': trial.suggest_int('min_child_weight', 2, 15),\n","        \"gamma\": trial.suggest_float(\"gamma\", 0.1, 1.0, log=True)\n","    }\n","    xgbc = XGBClassifier(n_jobs = 2,random_state=123, use_label_encoder=False, **params,\n","        objective = \"multi:softmax\", eval_metric  =\"error\") # multi:softmax이거 일듯\n","    score= cross_val_score(xgbc, X_train, y_train, cv=kfold, scoring=\"accuracy\")\n","    accuracy = score.mean()\n","    return accuracy\n","xgbc_study = optuna.create_study(direction=\"maximize\")\n","print(xgbc_study.optimize(xgb_objective, n_trials=10))\n","print(xgbc_study.best_trial.params)\n","print(xgbc_study.best_trial.values)\n","#3 optimization\n","optuna.visualization.plot_param_importances(xgbc_study)\n","opt_xgbc = XGBClassifier( \n","                        n_jobs= 2,\n","                        random_state=123,\n","                        booster=xgbc_study.best_params[\"booster\"],\n","                        n_estimators = xgbc_study.best_params[\"n_estimators\"],\n","                        learning_rate = xgbc_study.best_params[\"learning_rate\"],\n","                        max_depth = xgbc_study.best_params[\"max_depth\"],\n","                        subsample = xgbc_study.best_params[\"subsample\"],\n","                        colsample_bytree = xgbc_study.best_params[\"colsample_bytree\"],\n","                        reg_alpha = xgbc_study.best_params[\"reg_alpha\"],\n","                        reg_lambda = xgbc_study.best_params[\"reg_lambda\"],\n","                        use_label_encoder=False,\n","                        objective = \"multi:softmax\", # multi:softmax이거 일듯\n","                        eval_metric = \"error\")\n","opt_xgbc.fit(X_train, y_train)\n","print(accuracy_score(y_test, opt_xgbc.predict(X_test)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AIWDsuXfTL7A"},"outputs":[],"source":["#AdaBoost\n","import optuna, matplotlib.pyplot as plt\n","from sklearn.model_selection import KFold, cross_val_score\n","from sklearn.metrics import accuracy_score, precision_score, recall_score\n","from sklearn.ensemble import AdaBoostClassifier\n","#1 k-fold\n","kfold = KFold(n_splits=5, random_state=123, shuffle=True)\n","#2 adaboost\n","def ada_objective(trial):\n","    params = { #알고리즘은 디폴트값이 다른값들 보다 좋음, 다른거는 이항연산자임 파라미터가 없음 그래서 굳이 필없\n","        \"n_estimators\" : trial.suggest_int(\"n_estimators\", 50,500),\n","        \"learning_rate\" : trial.suggest_loguniform(\"learning_rate\", 1e-3, 8e-1)\n","    }\n","    ada = AdaBoostClassifier(n_jobs = -1, random_state=123,**params)\n","    score = cross_val_score(ada, X_train, y_train, cv=kfold, scoring=\"accuracy\")\n","    accuracy = score.mean()\n","    return accuracy\n","ada_study = optuna.create_study(direction=\"maximize\")\n","print(ada_study.optimize(ada_objective, n_trials=10))\n","print(ada_study.best_trial.params)\n","print(ada_study.best_trial.values)\n","#3 optimization\n","optuna.visualization.plot_param_importances(ada_study)\n","opt_ada = AdaBoostClassifier( \n","                        n_jobs= -1,\n","                        random_state=123,\n","                        n_estimators = ada_study.best_params[\"n_estimators\"],\n","                        learning_rate = ada_study.best_params[\"learning_rate\"]\n","                        )\n","opt_ada.fit(X_train, y_train)\n","print(accuracy_score(y_test, opt_ada.predict(X_test)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rOTgsgBbTL7A"},"outputs":[],"source":["#SVM\n","import optuna, matplotlib.pyplot as plt\n","from sklearn.model_selection import KFold, cross_val_score\n","from sklearn.metrics import accuracy_score, precision_score, recall_score\n","from sklearn.svm import SvcClassifier\n","#1 k-fold\n","kfold = KFold(n_splits=5, random_state=123, shuffle=True)\n","#SVC\n","def svc_objective(trial):\n","    params = {\n","        'kernel': trial.suggest_categorical('kernel', ['linear','rbf','sigmoid']),\n","        'C': trial.suggest_loguniform('C', 1e+0, 1e+2/2),\n","        'degree' :trial.suggest_int(\"degree\", 0,6),\n","        'gamma': trial.suggest_loguniform('gamma', 1e-3, 3.0)\n","    }\n","    svc = SvcClassifier(n_jobs = -1, random_state=123,**params)\n","    score = cross_val_score(svc, X_train, y_train, cv=kfold, scoring=\"accuracy\")\n","    accuracy = score.mean()\n","    return accuracy\n","svc_study = optuna.create_study(direction=\"maximize\")\n","print(svc_study.optimize(svc_objective, n_trials=10))\n","print(svc_study.best_trial.params)\n","print(svc_study.best_trial.values)\n","#3 optimization\n","optuna.visualization.plot_param_importances(svc_study)\n","opt_svc = AdaBoostClassifier( \n","                        n_jobs= -1,\n","                        random_state=123,\n","                        kernel = svc_study.best_params[\"kernel\"],\n","                        C = svc_study.best_params[\"C\"],\n","                        degree = svc_study.best_params[\"degree\"],\n","                        gamma = svc_study.best_params[\"gamma\"]\n","                        )\n","opt_svc.fit(X_train, y_train)\n","print(accuracy_score(y_test, opt_svc.predict(X_test)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rn49Za8sTL7B"},"outputs":[],"source":["# 분류\n","# K-최근접 이웃 : K-Nearest Neighbors\n","# 선형 회귀 : Linear Regression\n","# 로지스틱 회귀 : Logistic Regression\n","# 결정 트리 : Decision Tree\n","# 랜덤 포레스트 : Random Forest\n","# 점진적 부스팅 머신: GBM(Gradient boosting Machine)\n","# 서포트 벡터 머신 : SVM(Support Vector Machine)\n","# 신경망 : Neural Network"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"a6979645db105e21eace26815b8d6fa10f119ec4d0e7e598f032e7448139a62d"}}},"nbformat":4,"nbformat_minor":0}
